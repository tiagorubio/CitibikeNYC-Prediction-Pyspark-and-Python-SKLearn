{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ibmos2spark\n",
    "import pyspark\n",
    "from pyspark.sql import types as tp\n",
    "\n",
    "# @hidden_cell\n",
    "credentials = {\n",
    "    'endpoint': 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx',\n",
    "    'api_key': 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx',\n",
    "    'service_id': 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx',\n",
    "    'iam_service_endpoint': 'https://iam.ng.bluemix.net/oidc/token',\n",
    "    'bucket':'pilotbigdataprojectxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'\n",
    "}\n",
    "\n",
    "configuration_name = 'os_bf5780348bf84067aa71ba938d56f9a0_configs'\n",
    "cos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\n",
    "\n",
    "#change this to True to use a smaller file\n",
    "testRunInd=0 #run for whole training data 2016-2017\n",
    "#testRunInd=1 #run 2017Q4 smaller file\n",
    "#testRunInd=2 #run for test data 2018/01 and 2018/02\n",
    "\n",
    "stationId=''\n",
    "#stationId='83,64,288'\n",
    "\n",
    "\n",
    "if testRunInd==2:\n",
    "    sourceTripFile='2018Q1_citibike-tripdata.csv.bz2'\n",
    "    targetFile='TestData_2018Q1'\n",
    "    dateRange=\" '2018-01-01 00:00:00' and '2018-02-28 23:59:59' \"\n",
    "\n",
    "\n",
    "elif testRunInd==1:\n",
    "    sourceTripFile='2017Q4_citibike-tripdata.csv.bz2'\n",
    "    targetFile='TrainingData_2017Q4'\n",
    "    dateRange=\" '2017-10-01 00:00:00' and '2017-12-31 23:59:59' \"\n",
    "\n",
    "else:\n",
    "    sourceTripFile='2016-2017_citibike-tripdata.csv.bz2'\n",
    "    targetFile='TrainingData_2016-2017'\n",
    "    dateRange=\" '2016-01-01 00:00:00' and '2017-12-31 23:59:59' \"\n",
    "\n",
    "targetFile+='_Station-'+stationId.replace(',','-')+'.csv'\n",
    "weatherDataPath=cos.url('WeatherData_1261418.csv', credentials['bucket'])\n",
    "calendarDataPath=cos.url('calendar.csv',  credentials['bucket'])\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"BikeDataIntermediateFile\").getOrCreate()\n",
    "#spark.config('SHUFFLE_PARTITIONS', 24)\n",
    "#spark.conf.get(\"spark.sql.shuffle.partitions\")\n",
    "\n",
    "\n",
    "#SET spark.sql.shuffle.partitions=[num_tasks];\n",
    "#dfTripData.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prep Trip Data\n",
    "\n",
    "# Please read the documentation of PySpark to learn more about the possibilities to load data files.\n",
    "# PySpark documentation: https://spark.apache.org/docs/2.0.1/api/python/pyspark.sql.html#pyspark.sql.SparkSession\n",
    "# The SparkSession object is already initialized for you.\n",
    "# The following variable contains the path to your file on your IBM Cloud Object Storage.\n",
    "tripDataPath = cos.url(sourceTripFile,  credentials['bucket'])\n",
    "\n",
    "bikeSchema = tp.StructType([\n",
    " tp.StructField(\"tripduration\", tp.IntegerType() , True ),\n",
    " tp.StructField(\"starttime\", tp.TimestampType() , True ),\n",
    " tp.StructField(\"stoptime\", tp.TimestampType() , True ),\n",
    " tp.StructField(\"start station id\", tp.StringType() , True ),\n",
    " tp.StructField(\"start station name\", tp.StringType() , True ),\n",
    " tp.StructField(\"start station latitude\", tp.DoubleType() , True ),\n",
    " tp.StructField(\"start station longitude\", tp.DoubleType() , True ),\n",
    " tp.StructField(\"end station id\", tp.StringType() , True ),\n",
    " tp.StructField(\"end station name\", tp.StringType() , True ),\n",
    " tp.StructField(\"end station latitude\", tp.StringType() , True ),\n",
    " tp.StructField(\"end station longitude\", tp.StringType() , True ),\n",
    " tp.StructField(\"bikeid\", tp.StringType() , True ),\n",
    " tp.StructField(\"usertype\", tp.StringType() , True ),\n",
    " tp.StructField(\"birth year\", tp.StringType() , True ),\n",
    " tp.StructField(\"gender\", tp.StringType() , True )\n",
    "]\n",
    ")\n",
    "\n",
    "dfTripData = spark.read\\\n",
    "  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n",
    "  .option('header', 'true')\\\n",
    "  .option(\"timestampFormat\",\"MM/dd/yyyy HH:mm:ss\")\\\n",
    "  .schema(bikeSchema)\\\n",
    "  .load(tripDataPath)\n",
    "\n",
    "\n",
    "\n",
    "stationFilteEndr=''\n",
    "\n",
    "if stationId!='':\n",
    "    stationFilterEnd=\"\"\"\n",
    "   and `end station id` in (\"\"\" +stationId + \"\"\")\n",
    "   \"\"\"\"\n",
    "\n",
    "stationFilterStart=''\n",
    "if stationId!='':\n",
    "    stationFilterStart=\"\"\"\n",
    "   and `start station id` in (\"\"\" +stationId + \"\"\")\n",
    "   \"\"\"\"\n",
    "    \n",
    "dfTripData.createOrReplaceTempView(\"tripdata\")\n",
    "q1 = \"\"\" Select Year*100+Month as YearMonth,\n",
    "                StationID,\n",
    "                Year,\n",
    "                Month,\n",
    "                Day,\n",
    "                Hour,\n",
    "                DateHour,\n",
    "                InBike,\n",
    "                OutBike\n",
    "    from ( \n",
    "        SELECT\n",
    "            cast(date_format(starttime, 'yyyy-MM-dd HH:00:00') as timestamp) as DateHour,\n",
    "            cast(year(starttime) as smallint) as Year ,\n",
    "            cast(month(starttime) as tinyint) as Month ,\n",
    "            cast(day(starttime) as tinyint) as Day ,\n",
    "            cast(hour(starttime) as tinyint) as Hour ,\n",
    "            cast(`start station id` as smallint) as StationID,\n",
    "            cast(0 as smallint) as InBike,\n",
    "            cast(1 as smallint) as OutBike\n",
    "    \n",
    "        FROM\n",
    "          tripdata m\n",
    "        where starttime between \"\"\" +dateRange + stationFilterStart + \"\"\"\n",
    "        \n",
    "    \n",
    "    UNION ALL\n",
    "\n",
    "        SELECT\n",
    "            cast(date_format(stoptime, 'yyyy-MM-dd HH:00:00') as timestamp) as DateHour,\n",
    "            cast(year(stoptime) as smallint) as Year ,\n",
    "            cast(month(stoptime) as tinyint) as Month ,\n",
    "            cast(day(stoptime) as tinyint) as Day ,\n",
    "            cast(hour(stoptime) as tinyint) as Hour ,\n",
    "            cast(`end station id` as smallint) as StationID,\n",
    "            cast(1 as smallint) as InBike,\n",
    "            cast(0 as smallint) as OutBike\n",
    "    \n",
    "        FROM\n",
    "          tripdata m\n",
    "        where stoptime between \"\"\" +dateRange + stationFilterEnd + \"\"\"\n",
    "        \n",
    "    ) as TripDataTemp\n",
    "    \"\"\"\n",
    "\n",
    "dfTripDataTemp= spark.sql(q1)\n",
    "dfTripDataTemp.createOrReplaceTempView(\"TripDataTemp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prep Calendar and Weather data\n",
    "\n",
    "dfWeatherData = spark.read\\\n",
    "  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n",
    "  .option(\"timestampFormat\",\"yyyy-MM-dd HH:mm\")\\\n",
    "  .option(\"inferSchema\", \"true\")\\\n",
    "  .option('header', 'true')\\\n",
    "  .load(weatherDataPath)\n",
    "\n",
    "dfWeatherData.createOrReplaceTempView(\"WeatherData\")\n",
    "#dfWeatherData.printSchema()\n",
    "    \n",
    "qw = \"\"\"\n",
    "    Select distinct\n",
    "        DateHour\n",
    "        ,LAST_VALUE(HOURLYPRSENTWEATHERTYPE) over(partition by DateHour order by DateHour ) as HOURLYPRSENTWEATHERTYPE\n",
    "        ,LAST_VALUE(HOURLYPRSENTWEATHERTYPE_IND) over(partition by DateHour order by DateHour ) as HOURLYPRSENTWEATHERTYPE_IND\n",
    "        ,LAST_VALUE(HOURLYDRYBULBTEMPC) over(partition by DateHour order by DateHour ) as HOURLYDRYBULBTEMPC\n",
    "        ,LAST_VALUE(HOURLYWindSpeed) over(partition by DateHour order by DateHour ) as HOURLYWindSpeed\n",
    "        ,LAST_VALUE(DAILYSnowDepth) over(partition by DateHour order by DateHour ) as DAILYSnowDepth\n",
    "        ,LAST_VALUE(HOURLYPrecipT) over(partition by DateHour order by DateHour ) as HOURLYPrecip\n",
    "    from (\n",
    "        Select cast(date_format(DATE, 'yyyy-MM-dd HH:00:00') as timestamp) as DateHour,\n",
    "            coalesce(HOURLYPRSENTWEATHERTYPE,'') as HOURLYPRSENTWEATHERTYPE,\n",
    "            IF( HOURLYPRSENTWEATHERTYPE is not null, 1, 0) as HOURLYPRSENTWEATHERTYPE_IND,\n",
    "            coalesce(cast(regexp_replace(HOURLYDRYBULBTEMPC,'[^0-9.]', '') as double),0) as HOURLYDRYBULBTEMPC,\n",
    "            coalesce(HOURLYWindSpeed,0) as HOURLYWindSpeed,\n",
    "            coalesce(cast(regexp_replace(DAILYSnowDepth,'[^0-9.]', '') as double),0) as DAILYSnowDepth,\n",
    "            coalesce(cast(regexp_replace(HOURLYPrecip,'[^0-9.]', '') as double),0) as HOURLYPrecipT\n",
    "        from WeatherData\n",
    "        where REPORTTPYE='FM-15'\n",
    "        and DATE between \"\"\" +dateRange + \"\"\" \n",
    "        \n",
    "    )t1\n",
    "    \n",
    "\n",
    " \"\"\"    \n",
    "dfWeatherDataTemp = spark.sql(qw)\n",
    "dfWeatherDataTemp.createOrReplaceTempView(\"WeatherDataTemp\")\n",
    "#dfWeatherDataTemp.printSchema()\n",
    "#dfWeatherDataTemp.show()\n",
    "\n",
    "\n",
    "calendarSchema = tp.StructType([\n",
    " tp.StructField(\"year\", tp.IntegerType() , True ),\n",
    " tp.StructField(\"month\", tp.IntegerType() , True ),\n",
    " tp.StructField(\"day\", tp.IntegerType() , True ),\n",
    " tp.StructField(\"date\", tp.DateType() , True ),\n",
    " tp.StructField(\"weekday#\", tp.IntegerType() , True ),\n",
    " tp.StructField(\"business day\", tp.IntegerType() , True ),\n",
    " tp.StructField(\"Holiday\", tp.IntegerType() , True )\n",
    "]\n",
    ")\n",
    "\n",
    "dfCalendar = spark.read\\\n",
    "  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n",
    "  .schema(calendarSchema)\\\n",
    "  .option('header', 'true')\\\n",
    "  .load(calendarDataPath)\n",
    "\n",
    "dfCalendar.createOrReplaceTempView(\"Calendar\")\n",
    "#dfCalendar.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Group the data for the final file\n",
    "q  = \"\"\"\n",
    "select \n",
    "    bikeData.*,\n",
    "    InBike - OutBike as Result,\n",
    "    COALESCE(wc.HOURLYPRSENTWEATHERTYPE, LAST_VALUE(wc.HOURLYPRSENTWEATHERTYPE, TRUE) OVER(ORDER BY bikeData.DateHour ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)) as HOURLYPRSENTWEATHERTYPE,\n",
    "    COALESCE(wc.HOURLYPRSENTWEATHERTYPE_IND, LAST_VALUE(wc.HOURLYPRSENTWEATHERTYPE_IND, TRUE) OVER(ORDER BY bikeData.DateHour ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)) as HOURLYPRSENTWEATHERTYPE_IND,\n",
    "    COALESCE(wc.HOURLYDRYBULBTEMPC, LAST_VALUE(wc.HOURLYDRYBULBTEMPC, TRUE) OVER(ORDER BY bikeData.DateHour ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)) as HOURLYDRYBULBTEMPC,\n",
    "    COALESCE(wc.HOURLYWindSpeed, LAST_VALUE(wc.HOURLYWindSpeed, TRUE) OVER(ORDER BY bikeData.DateHour ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)) as HOURLYWindSpeed,\n",
    "    COALESCE(wc.DAILYSnowDepth, LAST_VALUE(wc.DAILYSnowDepth, TRUE) OVER(ORDER BY bikeData.DateHour ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)) as DAILYSnowDepth,\n",
    "    COALESCE(wc.HOURLYPrecip, LAST_VALUE(wc.HOURLYPrecip, TRUE) OVER(ORDER BY bikeData.DateHour ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)) as HOURLYPrecip,\n",
    "    c.`business day` as workingDay\n",
    "    \n",
    "    from (\n",
    "\n",
    "    select \n",
    "            YearMonth,\n",
    "            Hour,\n",
    "            StationID,\n",
    "            Year,\n",
    "            Month,\n",
    "            Day,\n",
    "            DateHour,\n",
    "            to_date(DateHour) as DATE,\n",
    "            sum(InBike) as InBike,\n",
    "            sum(OutBike) as OutBike\n",
    "\n",
    "    from TripDataTemp as Final\n",
    "\n",
    "    group by YearMonth,\n",
    "        Hour,\n",
    "        StationID,\n",
    "        Year,\n",
    "        Month,\n",
    "        Day,\n",
    "        DateHour\n",
    "    order by YearMonth,\n",
    "        Hour,\n",
    "        StationID,\n",
    "        Year,\n",
    "        Month,\n",
    "        Day,\n",
    "        DateHour\n",
    "    ) bikeData\n",
    "    \n",
    "    left join WeatherDataTemp wc\n",
    "    on bikeData.DateHour = wc.DateHour\n",
    "    \n",
    "    left join Calendar c\n",
    "    on bikeData.DATE=c.date\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "dfTrainingData = spark.sql(q)\n",
    "#dfTrainingData.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the file as a single partition\n",
    "\n",
    "savePath = cos.url(targetFile,  credentials['bucket'])\n",
    "dfTrainingData.repartition(1).sortWithinPartitions(\"DateHour\").write.csv(savePath, mode ='overwrite', header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename the PART- files\n",
    "import ibm_boto3\n",
    "from botocore.client import Config\n",
    "\n",
    "credentialsBoto = {\n",
    "    'service_name':'s3',\n",
    "    'endpoint_url': credentials['endpoint'],\n",
    "    'ibm_api_key_id': credentials['api_key'],\n",
    "    'config': Config(signature_version='oauth'),\n",
    "    'ibm_auth_endpoint': credentials['iam_service_endpoint']}\n",
    "\n",
    "resource = ibm_boto3.resource(**credentialsBoto)\n",
    "\n",
    "client = ibm_boto3.client(**credentialsBoto)\n",
    "\n",
    "#search for PART- the files\n",
    "Prefix=targetFile+'/part-'\n",
    "fileList = client.list_objects_v2(Bucket= credentials['bucket'],Prefix=Prefix)\n",
    "\n",
    "i=0\n",
    "#if found, run through the list and copy them with a new name and delete the old one.\n",
    "try:\n",
    "    for files in fileList[\"Contents\"]:\n",
    "        fileName = files[\"Key\"]\n",
    "        #print(fileName)\n",
    "\n",
    "        if fileName.startswith(Prefix):\n",
    "            i+=1\n",
    "        \n",
    "            if i==1:\n",
    "                newFileName=targetFile\n",
    "            else:\n",
    "                newFileName=targetFile[:-4]+'%s.csv'%(i)\n",
    "\n",
    "            #print(newFileName)\n",
    "            responseSave = resource.Bucket(fileList[\"Name\"]).Object(newFileName).copy_from(CopySource=fileList[\"Name\"]+'/'+fileName)\n",
    "            responseDelete = resource.Bucket(fileList[\"Name\"]).Object(fileName).delete()\n",
    "            #print(str(responseSave))\n",
    "            #print(str(responseDelete))\n",
    "except KeyError as err:\n",
    "    print('File not found while renaming. Error: '+str(err))\n",
    "\n",
    "#delete /_SUCCESS file\n",
    "responseDelete = resource.Bucket( credentials['bucket']).Object(targetFile+'/_SUCCESS').delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 with Spark 2.1",
   "language": "python",
   "name": "python3-spark21"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
